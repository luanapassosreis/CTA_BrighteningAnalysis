{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a887d56b-feaf-4506-a643-3c47415a7db5",
   "metadata": {},
   "source": [
    "# Init\n",
    "\n",
    "This is a init file with all the imports, functions and dataframes that are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f26480-9065-4316-8f75-d2e34f5b1864",
   "metadata": {},
   "source": [
    "**Change path here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0403d460-3eeb-47a6-ac6e-61791b078b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path for the downloaded folder\n",
    "\n",
    "PATH = '/Users/luanareis/Documents/IC/CTA_BrighteningAnalysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6822b91c-3f7d-41cf-af27-0f18e9b716d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path for the folder with the downloaded lightcurves from the catalog\n",
    "\n",
    "PATH_3DAY_LC = f'{PATH}/lightcurve_downloader/downloaded_lightcurves/3day_lightcurves'\n",
    "PATH_WEEKLY_LC = f'{PATH}/lightcurve_downloader/downloaded_lightcurves/weekly_lightcurves'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc89cf-dc6c-4007-a643-c9ca265d2953",
   "metadata": {},
   "source": [
    "Printing the paths to help using it in another files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e501e320-5ef5-4b61-ad44-a2e6fcf3dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you run __init__.ipynb, you upload the imports, as well as the paths, functions and dataframes necessary for the analysis.\n",
      "\n",
      "################### IMPORTANT PATHS ###################\n",
      "\n",
      "--> PATH : for the main folder CTA_BrighteningAnalysis downloaded from Github\n",
      "\n",
      "For the downloaded lightcurves from the catalog folder, use:\n",
      "-> PATH_3DAY_LC : 3-day bin lightcurves\n",
      "-> PATH_WEEKLY_LC : weekly bin lightcurves\n"
     ]
    }
   ],
   "source": [
    "print('When you run __init__.ipynb, you upload the imports, as well as the paths, functions and dataframes necessary for the analysis.')\n",
    "\n",
    "print('\\n################### IMPORTANT PATHS ###################')\n",
    "\n",
    "print('\\n--> PATH : for the main folder CTA_BrighteningAnalysis downloaded from Github')\n",
    "\n",
    "print('\\nFor the downloaded lightcurves from the catalog folder, use:')\n",
    "\n",
    "print('-> PATH_3DAY_LC : 3-day bin lightcurves')\n",
    "print('-> PATH_WEEKLY_LC : weekly bin lightcurves')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45b5fb-0b5f-41ba-9349-ddf46e2a2e1d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897c141b-556c-437f-8232-1872ebd52798",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Astro imports\n",
    "\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from astropy.table import QTable\n",
    "import astropy.units as u\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "from astropy import units as u\n",
    "\n",
    "from astropy.time import Time,TimeUnix\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1bbbab-e47c-421f-8ac9-64dac97e3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.optimize as sp\n",
    "import scipy.odr.odrpack as odrpack\n",
    "from scipy import signal, integrate\n",
    "from scipy.fft import fft, fftfreq\n",
    "import numpy as np\n",
    "import statistics\n",
    "import csv\n",
    "import math\n",
    "from scipy.fft import fft, fftfreq\n",
    "import glob\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04526e8-01bc-4340-88ca-fe3e113fdc1e",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "- convert_MET_UTC: converts the x-axis array of the lightcurve from MET to UTC - https://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data/Time_in_ScienceTools.html\n",
    "\n",
    "- dNdE: log-normal representation (LogParabola under SpectrumType in the FITS table) and a simple power-law form for $\\beta = 0$. - https://arxiv.org/pdf/1902.10045.pdf\n",
    "\n",
    "$$ \\frac{dN}{dE} =  K\\left ( \\frac{E}{E_{0}} \\right )^{-\\alpha \\ - \\beta \\ log(E/E_{0})} $$\n",
    "\n",
    "- diff_flux: Differential Flux $\\nu F_{\\nu }$, particle flux density per unit energy incident on a surface $[erg\\ cm^{-2} s^{-1}]$.\n",
    "\n",
    "$$ \\nu F_{\\nu } = E^{2} \\  \\frac{dN}{dE} = E^{2} \\  K\\left ( \\frac{E}{E_{0}} \\right )^{-\\alpha \\ - \\beta \\ log(E/E_{0})} $$\n",
    "\n",
    "- extract_data_from_file: given the bins (Weekly/ 3-days), returns the extracted data from the .json file, returning many arrays.\n",
    "\n",
    "- synchr_type_dataframe: given the file_name, returns the dataframe that the file is contained based on its Synchroton Type.\n",
    "\n",
    "- extract_parameters_from_table: parameters from FITS table of 10-year observations $ [0.1-100\\ GeV\\ ph\\ cm^{−2}\\ s^{−1}] $, by using\n",
    "\n",
    "\n",
    "$$ \\int \\frac{dN}{dE} dE \\Rightarrow  \\left [ \\int \\frac{dN}{dE} dE \\right ] = \\frac{ph}{cm^{2} \\ s}[0.1-100 \\ GeV] $$\n",
    "\n",
    "\n",
    "- relative_difference: relative difference between the flux and the spectrum flux\n",
    "\n",
    "$$ relative.difference = \\frac{ \\left | \\ spectrum.flux - average.flux \\ \\right | }{ average.flux } $$\n",
    "\n",
    "- filter_percentage: filtering the sources with a % difference between the average and the spectrum flux, if the relative differece in a percentage is smaller than the chosen value, select it when True.\n",
    "\n",
    "- calculate_average_flux_when_fluxUL_is_zero: calculate average flux when setting Upper Limits as flux = 0 \n",
    "\n",
    "-  plot_SED: plot of the Spectral Energy Distribution, the differential flux $\\nu\\ F_{\\nu}$ in units of '$erg \\ cm^{-2} \\ s^{-1}$' as a function of the Energy in '$GeV$'.\n",
    "\n",
    "- plot_lc: plot of the LightCurve, the Photon Flux in units of '$ 0.1 - 100 \\ GeV \\ ph \\ cm^{-2} \\ s^{-1}$' as a fuction of Time in '$MET \\ s$'. If you want to change it to UTC, you can use the function convert_MET_UTC to convert the time array and change the name of the x-axis :)\n",
    "\n",
    "- plot_histogram: make Histograms of the flux_proportion of the Flux/Spectrum.Flux for each flux point in each lightcurve of the catalog in a y log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f493ff-4d06-40d2-b228-ce1b8f93be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ---------- convert time from MET to UTC ----------\n",
    "def convert_MET_UTC(time_MET):\n",
    "    ########## Description ##########\n",
    "    # more info on time in LAT:\n",
    "    # https://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data/Time_in_ScienceTools.html\n",
    "    # ---------- Input ----------\n",
    "    # time_MET : time using MET (seconds) from January 1st, 2001 until now  (1D-array)\n",
    "    # ---------- Output ----------\n",
    "    # time in UTC                                                           (1D-array)\n",
    "    #################################\n",
    "    \n",
    "    # changing data to Unix (referencial of 1970)\n",
    "    time_Unix = Time(time_MET, format='unix', scale='utc')\n",
    "\n",
    "    # finding how many seconds are there from 1970 to January 1st, 2001\n",
    "    time_difference = Time('2001-01-01', format='iso', scale='utc')\n",
    "    time_difference.format = 'unix'\n",
    "    time_difference.value\n",
    "\n",
    "    # increased the diffence of time between 2001 and 1970 to my data\n",
    "    time_MET_copy = np.copy(time_MET)\n",
    "    time_MET_copy += time_difference.value\n",
    "\n",
    "    # transform into year: format='iso'\n",
    "    time_Unix = Time(time_MET_copy, format='unix', scale='utc')\n",
    "    time_Unix.format = 'iso'\n",
    "    time_Unix\n",
    "\n",
    "    # list with only date values, year, month and day\n",
    "    time_UTC = []\n",
    "    for i in range(len(time_Unix.value)):\n",
    "        time_UTC.append(datetime.strptime(time_Unix.value[i][:10], '%Y-%m-%d'))\n",
    "\n",
    "    return time_UTC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- log-normal representation ----------\n",
    "def dNdE(E, E_0, K, alpha, beta=None):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # E     : value of the energy                               (1D-array)\n",
    "    # E_0   : value of the pivot_Energy in erg                  (float)\n",
    "    # K     : value of the flux_Density in erg-1 cm-2 s-1       (float)\n",
    "    # alpha : Index                                             (float)\n",
    "    # beta  : Beta (only for LogParabola SED_class)             (float)\n",
    "    # ---------- Output ----------\n",
    "    # log-normal representation value dN/dE in 'erg-1 cm-2 s-1' (1D-array)\n",
    "    #################################\n",
    "    \n",
    "    if (beta == None):\n",
    "        Y = K * ((E/E_0)**(-alpha))\n",
    "    else:\n",
    "        Y = K * ((E/E_0)**(-alpha - beta * np.log(E/E_0)))       \n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- Differential Flux ----------\n",
    "def diff_flux(E, E_0, K, alpha, beta=None):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # E     : value of the energy                          (1D-array)\n",
    "    # E_0   : value of the pivot_Energy in erg             (float)\n",
    "    # K     : value of the flux_Density in erg-1 cm-2 s-1  (float)\n",
    "    # alpha : Index                                        (float)\n",
    "    # beta  : Beta (only for LogParabola SED_class)        (float)\n",
    "    # ---------- Output ----------\n",
    "    # differential flux value E^2 dN/dE in 'erg cm-2 s-1'  (1D-array)\n",
    "    #################################\n",
    "    \n",
    "    if (beta == None):\n",
    "        Y = E**2 * K * ((E/E_0)**(-alpha))\n",
    "    else:\n",
    "        Y = E**2 * K * ((E/E_0)**(-alpha - beta * np.log(E/E_0)))     \n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- extract data from .json file ----------\n",
    "def extract_data_from_file(bins, file_name):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # file_name : name of .json file                          (string)\n",
    "    # ---------- Output ----------\n",
    "    # name : name of the source                               (string)\n",
    "    # time, flux : axis for Light Curve                       (array)\n",
    "    # time_error, flux_low_error, flux_high_error : error     (array)\n",
    "    # time_upper_lim, flux_upper_lim : upper limits           (array)\n",
    "    #################################\n",
    "    \n",
    "    if (bins == 'weekly'):\n",
    "        file = open(f'{PATH_WEEKLY_LC}/{file_name}')\n",
    "    if (bins == '3-days'):\n",
    "        file = open(f'{PATH_3DAY_LC}/{file_name}')\n",
    "    \n",
    "    name = file_name[5:-5]\n",
    "    \n",
    "    # returns JSON object as a dictionary\n",
    "    data = json.load(file)\n",
    "    \n",
    "    # creating arrays to store the data\n",
    "    time = np.array(data['flux'])[:,0]     # 'flux' [i][0]\n",
    "    flux = np.array(data['flux'])[:,1]     # 'flux' [i][1]\n",
    "    \n",
    "    time_error      = np.array(data['flux_error'])[:,0]     # 'flux_error' [i][0]\n",
    "    flux_low_error  = np.array(data['flux_error'])[:,1]     # 'flux_error' [i][1]  - lower flux edge\n",
    "    flux_high_error = np.array(data['flux_error'])[:,2]     # 'flux_error' [i][2]  - high edge\n",
    "    \n",
    "    time_upper_lim  = np.array(data['flux_upper_limits'])[:,0]     # 'flux_upper_limits' [i][0]\n",
    "    flux_upper_lim  = np.array(data['flux_upper_limits'])[:,1]     # 'flux_upper_limits' [i][1]\n",
    "    \n",
    "    return name, time, flux, time_error, flux_low_error, flux_high_error, time_upper_lim, flux_upper_lim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- Which dataframe to use based on the Synchroton Type ----------\n",
    "def synchr_type_dataframe(file_name):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # file_name : name of the file in .json         (string)\n",
    "    # ---------- Output ----------\n",
    "    # dataframe : which dataframe to use            (dataframe)\n",
    "    #################################\n",
    "    \n",
    "    ## ---------- Low Synchrotron Peak ----------\n",
    "    if (dfLSP['Source_Name'] == file_name[5:-5]).any():\n",
    "        dataframe = dfLSP\n",
    "\n",
    "    ## ---------- Intermediate Synchrotron Peak ----------\n",
    "    if (dfISP['Source_Name'] == file_name[5:-5]).any():\n",
    "        dataframe = dfISP\n",
    "        \n",
    "    ## ---------- High Synchrotron Peak ----------\n",
    "    if (dfHSP['Source_Name'] == file_name[5:-5]).any():\n",
    "        dataframe = dfHSP\n",
    "        \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- parameters from FITS table of 10-year observations ----------\n",
    "def extract_parameters_from_table(name, dataframe, flux):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # name : name of the source                                (string)\n",
    "    # dataframe : which df to use                              (dataframe)\n",
    "    # flux : flux of the source from .json file                (array)\n",
    "    # ---------- Output ----------\n",
    "    # E : energy                                               (array)\n",
    "    # differential_flux : differential flux                    (array)\n",
    "    # spec_type : spectrum type                                (string)\n",
    "    # flux_from_spectrum : integrated flux from the SED        (float)\n",
    "    #################################\n",
    "\n",
    "    # defining Energy in 'erg'\n",
    "    E = ((np.logspace(np.log10(0.1), np.log10(100), 100) * u.GeV).to('erg')).value     # log scale of Energy in erg\n",
    "\n",
    "    # get the index of each one\n",
    "    index = dataframe[dataframe['Source_Name'] == name].index\n",
    "\n",
    "    for i in index:\n",
    "\n",
    "        # Pivot_Energy in erg\n",
    "        E0 = ((dataframe.loc[i,'Pivot_Energy'] * u.MeV).to('erg')).value\n",
    "\n",
    "        ## ---------- PowerLaw ----------\n",
    "        if dataframe.loc[i,'SpectrumType'] == 'PowerLaw         ':\n",
    "            spec_type = \"PowerLaw\"\n",
    "\n",
    "            # PL_Flux_Density in erg-1 cm-2 s-1\n",
    "            K = ((dataframe.loc[i,'PL_Flux_Density'] * u.MeV**-1 * u.cm**-2 * u.s**-1).to('erg-1 cm-2 s-1')).value\n",
    "            # PL_Index\n",
    "            alpha = dataframe.loc[i,'PL_Index']\n",
    "\n",
    "            differential_flux = diff_flux(E, E0, K, alpha)\n",
    "\n",
    "            integrateflux = integrate.quad(lambda x: K * ((x/E0)**(-alpha)),\n",
    "                                           (0.1*u.GeV).to('erg').value, (100*u.GeV).to('erg').value)\n",
    "\n",
    "            flux_from_spectrum = integrateflux[0] # returning only the first value of integrateflux\n",
    "\n",
    "        ## ---------- LogParabola ----------\n",
    "        elif dataframe.loc[i,'SpectrumType'] == 'LogParabola      ':\n",
    "            spec_type = \"LogParabola\"\n",
    "\n",
    "            # LP_Flux_Density in erg-1 cm-2 s-1\n",
    "            K = ((dataframe.loc[i,'LP_Flux_Density'] * u.MeV**-1 * u.cm**-2 * u.s**-1).to('erg-1 cm-2 s-1')).value\n",
    "            # LP_Index\n",
    "            alpha = dataframe.loc[i,'LP_Index']\n",
    "            # LP_beta\n",
    "            beta = dataframe.loc[i,'LP_beta']\n",
    "\n",
    "            differential_flux = diff_flux(E, E0, K, alpha, beta)\n",
    "\n",
    "            integrateflux = integrate.quad(lambda x: K * ((x/E0)**(-alpha - beta * np.log(x/E0))), \n",
    "                                          (0.1*u.GeV).to('erg').value, (100*u.GeV).to('erg').value)\n",
    "\n",
    "            flux_from_spectrum = integrateflux[0] # returning only the first value of integrateflux\n",
    "\n",
    "        ## ---------- in case there is an error ----------\n",
    "        else:\n",
    "            print('### error ###')\n",
    "    \n",
    "        \n",
    "    return E, differential_flux, spec_type, flux_from_spectrum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- relative difference between the flux and spectrum flux ----------\n",
    "def relative_difference(flux, flux_from_spectrum):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # flux : the light curve flux                                                         (float)\n",
    "    # flux_from_spectrum : integrated flux from the 10-years SED                          (float)\n",
    "    # ---------- Output ----------\n",
    "    # relative difference : relative difference between the average and integrated flux   (float)\n",
    "    #################################\n",
    "    \n",
    "    average_flux = np.average(flux)\n",
    "    \n",
    "    rel_difference = np.abs(flux_from_spectrum - average_flux) / average_flux\n",
    "    \n",
    "    return rel_difference\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- filtering the sources with a % difference between the average and spectrum flux ----------\n",
    "def filter_percentage(name, flux, flux_from_spectrum, percentage):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # name : name of the source                                     (string)\n",
    "    # flux : the light curve flux                                   (float)\n",
    "    # flux_from_spectrum : integrated flux from the 10-years SED    (float)\n",
    "    # percentage : percentage we want to filter                     (float)\n",
    "    # ---------- Output ----------\n",
    "    # True / False (boolean) : if we include the source or not      (float)\n",
    "    #################################\n",
    "    \n",
    "    rel_difference = relative_difference(flux, flux_from_spectrum) *100\n",
    "    \n",
    "    print(f'Source {name} percentage:', np.round(rel_difference))\n",
    "    \n",
    "    if (rel_difference <= percentage):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## ---------- calculate average when setting Upper Limits as flux = 0 ----------\n",
    "def calculate_average_flux_when_fluxUL_is_zero(flux, flux_upper_lim):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # flux: the source's flux points from the LC       (array)\n",
    "    # flux_upper_lim : flux of the upper limits        (array)\n",
    "    # ---------- Output ----------\n",
    "    # average_flux_withUL: average flux including UL   (float)\n",
    "    #################################\n",
    "    \n",
    "    ## defining a FluxUL = 0 and weighting the average of the LC flux to compare if it's the same\n",
    "    flux_UL = 0\n",
    "    n = len(flux_upper_lim)\n",
    "    # average_flux_withUL = (np.sum(flux) + n*flux_UL ) / (len(flux) + n)   # n*flux_UL = n*0 = 0\n",
    "    average_flux_withUL = (np.sum(flux)) / (len(flux) + n)\n",
    "    \n",
    "    return average_flux_withUL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- plot Spectral Energy Distribution ----------\n",
    "def plot_SED(E, differential_flux, power, name, spec_type, x_dlim, x_ulim, y_dlim, y_ulim):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # E : energy created by logspace from 0.1 to 100 GeV,\n",
    "    # given only the value in 'erg'                             (array)\n",
    "    # differential_flux : differential flux                     (array)\n",
    "    # power : power of the differential_flux                    (int)\n",
    "    # name  : name of the source                                (string)\n",
    "    # spect_type : Spectrum Type (logP/ powL)                   (string)\n",
    "    # x_dlim, x_ulim : limits of x axis                         (float)\n",
    "    # y_dlim, y_ulim : limits of y axis                         (float)\n",
    "    # ---------- Output ----------\n",
    "    # \n",
    "    #################################\n",
    "\n",
    "    plt.figure(figsize=(7,5), dpi=100)\n",
    "\n",
    "    plt.plot(((E*u.erg).to('GeV')).value, differential_flux*10**(power), '+', markersize=2, color='black')\n",
    "    plt.plot(((E*u.erg).to('GeV')).value, differential_flux*10**(power), '--', linewidth=0.4, color='black')\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "    plt.xlim(x_dlim, x_ulim)\n",
    "    plt.ylim(y_dlim, y_ulim)\n",
    "\n",
    "    \n",
    "    plt.title(f'4FGL+{name} - {spec_type} Spectrum', fontsize='large')\n",
    "    plt.ylabel(r'$\\nu\\ F_{\\nu}$ [ $ 10^{- %d } $ erg $cm^{-2}$ $s^{-1}$]' % (power), fontsize=12)\n",
    "    plt.xlabel('Energy (GeV)', fontsize=12)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- plot LightCurve ----------\n",
    "def plot_lc(name, time, flux, time_error, flux_low_error, flux_high_error,\n",
    "            time_upper_lim, flux_upper_lim, ylim, flux_from_spectrum):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # name: name of the source                                (string)\n",
    "    # time, flux : axis for Light Curve                       (array)\n",
    "    # time_error, flux_low_error, flux_high_error : error     (array)\n",
    "    # time_upper_lim, flux_upper_lim : upper limits           (array)\n",
    "    # ylim: limit of the y axis                               (float)\n",
    "    # flux_from_spectrum : integrated flux over 10-year SED   (float)\n",
    "    # ---------- Output ----------\n",
    "    # \n",
    "    #################################\n",
    "    \n",
    "    ## defining a relative difference between the simple average of LC points and the spectrum flux\n",
    "    difference = relative_difference(flux, flux_from_spectrum)\n",
    "    # print('difference:', difference)\n",
    "    \n",
    "    average_flux_withUL = calculate_average_flux_when_fluxUL_is_zero(flux, flux_upper_lim)\n",
    "    # ---- Input: flux, flux_upper_lim\n",
    "    # ---- Output: average_flux_withUL\n",
    "    \n",
    "    print('\\n##########')\n",
    "    print('(units 0.1-100 GeV ph cm-2 s-1)')\n",
    "    print('\\nFlux from Spectrum:', flux_from_spectrum)\n",
    "    print('Average LC Flux Points:', np.average(flux))\n",
    "    print('Average LC + Flux_UL=0:', average_flux_withUL)\n",
    "    \n",
    "    print('\\n----- Relative Difference: -----')\n",
    "    \n",
    "    print('\\nFlux from Spectrum vs. Average LC Flux Points:', format(np.abs(flux_from_spectrum - np.average(flux)) / flux_from_spectrum, '.4f'))\n",
    "    print('\\nFlux from Spectrum vs. Average LC + Flux_UL=0:', format(np.abs(flux_from_spectrum - average_flux_withUL) / flux_from_spectrum, '.4f'))\n",
    "    print('\\nAverage LC Flux Points vs, Average LC + Flux_UL=0:', format(np.abs(np.average(flux) - average_flux_withUL) / average_flux_withUL, '.4f'))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(17,5), dpi=300)\n",
    "\n",
    "    # flux\n",
    "    plt.plot(time, flux, '.', markersize=10, label='Flux Points')\n",
    "    plt.plot(time, flux, linewidth=0.4, color='black')\n",
    "\n",
    "    # upper_limits\n",
    "    plt.plot(time_upper_lim, flux_upper_lim, 'v', color='gray', markersize=3, alpha=0.45, label='Upper Limits')\n",
    "\n",
    "    # error_bar\n",
    "    # plt.plot(time_error, flux_low_error, 'x', color='green', markersize=3, alpha=0.3)  # visualize the error\n",
    "    # plt.plot(time_error, flux_high_error, 'x', color='red', markersize=3, alpha=0.3)   # visualize the error\n",
    "    plt.errorbar(time, flux, yerr=flux_high_error-flux, linewidth=0.2, color='black', alpha=0.9)\n",
    "    plt.errorbar(time, flux, yerr=flux-flux_low_error, linewidth=0.2, color='black', alpha=0.9)\n",
    "\n",
    "    # comparing fluxes\n",
    "    plt.hlines(y=flux_from_spectrum, xmin=np.min(time), xmax=np.max(time), linewidth=2, linestyles='dashed', color='green', label='Flux from Spectrum')\n",
    "    plt.hlines(y=np.average(flux), xmin=np.min(time), xmax=np.max(time), linewidth=2, linestyles='dashed', color='blue', label='Average LC Flux Points')\n",
    "    plt.hlines(y=average_flux_withUL, xmin=np.min(time), xmax=np.max(time), linewidth=2, linestyles='dashed', color='red', label='Average LC + Flux_UL=0')\n",
    "\n",
    "    # legend\n",
    "    plt.legend() # f'difference: {difference}'\n",
    "    \n",
    "    plt.ylim(0, ylim)\n",
    "    plt.title(f'4FGL+{name} Light Curve', fontsize='xx-large')\n",
    "    plt.ylabel('Photon Flux (0.1-100 GeV ph $cm^{-2}$ $s^{-1}$)', fontsize=12)\n",
    "    plt.xlabel('Time (MET s)', fontsize=12) # Date (UTC)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ---------- make Histograms ----------\n",
    "def plot_histogram(flux_proportion, bins1, bins2, x_low_lim, x_high_lim):\n",
    "    ########## Description ##########\n",
    "    # ---------- Input ----------\n",
    "    # flux_proportion : flux / integrated flux                 (array)\n",
    "    # bins1: name of the source                                (int)\n",
    "    # bins2 : axis for Light Curve                             (int)\n",
    "    # x_low_lim: inferior limit of x axis                      (float)\n",
    "    # x_high_lim: superior limit of x axis                     (float)\n",
    "    # ---------- Output ----------\n",
    "    # \n",
    "    #################################\n",
    "\n",
    "    n, bins, patches = plt.hist(flux_proportion, bins1, density=True, facecolor='g', alpha=0.75)\n",
    "    n, bins, patches = plt.hist(flux_proportion, bins2, density=True, facecolor='red', alpha=0.75)\n",
    "\n",
    "    plt.xlabel('Flux / Spectrum Flux')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.xlim(x_low_lim, x_high_lim)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39017866-5846-4716-9f7c-2731412f7677",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c9bf3a-b153-4e1c-8635-ba576d0f2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- Low Synchrotron Peak ----------\n",
    "dfLSP = pd.read_csv(f'{PATH}/4LAC_catalog_generator_v2/LSP_data.csv')\n",
    "## ---------- Intermediate Synchrotron Peak ----------\n",
    "dfISP = pd.read_csv(f'{PATH}/4LAC_catalog_generator_v2/ISP_data.csv')\n",
    "## ---------- High Synchrotron Peak ----------\n",
    "dfHSP = pd.read_csv(f'{PATH}/4LAC_catalog_generator_v2/HSP_data.csv')\n",
    "\n",
    "## ---------- Whole Dataframe ----------\n",
    "dat = pd.read_csv(f'{PATH}/4LAC_catalog_generator_v2/whole_dataframe.csv')\n",
    "\n",
    "## ---------- Frac_Variability Dataframe ----------\n",
    "df_Fvar = pd.read_csv(f'{PATH}/4LAC_catalog_generator_v2/Fvar_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5284b9-d43f-4b6e-9a58-73506256f940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### DATAFRAMES ######################\n",
      "\n",
      "# of sources in the dataframes:\n",
      "dfLSP: 1538\n",
      "dfISP: 508\n",
      "dfHSP: 548\n",
      "---> total: 2594\n",
      "\n",
      "dat: 3511\n",
      "\n",
      "Name of the dataframe with the Fractional Variability values: df_Fvar\n"
     ]
    }
   ],
   "source": [
    "print('\\n##################### DATAFRAMES ######################')\n",
    "\n",
    "print('\\n# of sources in the dataframes:')\n",
    "print('dfLSP:', len(dfLSP))\n",
    "print('dfISP:', len(dfISP))\n",
    "print('dfHSP:', len(dfHSP))\n",
    "print('---> total:', len(dfLSP) + len(dfISP) + len(dfHSP))\n",
    "\n",
    "print('\\ndat:', len(dat))\n",
    "\n",
    "print('\\nName of the dataframe with the Fractional Variability values: df_Fvar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf708b-243e-4cff-867b-5e66b786fe3a",
   "metadata": {},
   "source": [
    "## Filtering the .json files that are on each table (filter by Synchrotron Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14a459-1b80-4018-9332-25a2a06a32ed",
   "metadata": {},
   "source": [
    "### 3 days bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c376335-ebc8-4647-883e-a8f9e23dccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select every file that ends with .json in folder\n",
    "lc_3day = glob.glob(PATH_3DAY_LC + '/*.json')\n",
    "\n",
    "## Create empty list to store dataframes\n",
    "lc_downloaded_3days = []\n",
    "\n",
    "## Loop through list of files and read each one into a dataframe and append to list\n",
    "for file in lc_3day:\n",
    "    \n",
    "    ## Removing the path from the file name\n",
    "    stripped_file = file.lstrip(f'{PATH_3DAY_LC}/')\n",
    "    \n",
    "    ## Append to list of the lightcurves\n",
    "    lc_downloaded_3days.append(stripped_file)\n",
    "    \n",
    "lc_downloaded_3days.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93fc220-e533-433e-ab22-b98b6488be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list for each Synchrotron Type\n",
    "lc_downloaded_LSP_3days = [] # low\n",
    "lc_downloaded_ISP_3days = [] # intermediate\n",
    "lc_downloaded_HSP_3days = [] # high\n",
    "\n",
    "for file in lc_downloaded_3days:\n",
    "    ## ---------- Low Synchrotron Peak ----------\n",
    "    if (dfLSP['Source_Name'] == file[5:-5]).any():\n",
    "        lc_downloaded_LSP_3days.append(f'4FGL+{file[5:-5]}.json')\n",
    "        \n",
    "    ## ---------- Intermediate Synchrotron Peak ----------\n",
    "    if (dfISP['Source_Name'] == file[5:-5]).any():\n",
    "        lc_downloaded_ISP_3days.append(f'4FGL+{file[5:-5]}.json')\n",
    "        \n",
    "    ## ---------- High Synchrotron Peak ----------\n",
    "    if (dfHSP['Source_Name'] == file[5:-5]).any():\n",
    "        lc_downloaded_HSP_3days.append(f'4FGL+{file[5:-5]}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "094cb452-11d8-415a-a75d-1c77e1cbe422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############### LIGHTCURVES FROM FOLDER ###############\n",
      "\n",
      "--------------- 3-day bin ---------------\n",
      "\n",
      "# of lightcurves from the folder in the array:\n",
      "lc_downloaded_LSP_3days: 465\n",
      "lc_downloaded_ISP_3days: 60\n",
      "lc_downloaded_HSP_3days: 66\n",
      "---> total: 591\n",
      "\n",
      "lc_downloaded_3days: 683\n"
     ]
    }
   ],
   "source": [
    "print('\\n############### LIGHTCURVES FROM FOLDER ###############')\n",
    "\n",
    "print('\\n--------------- 3-day bin ---------------')\n",
    "\n",
    "print('\\n# of lightcurves from the folder in the array:')\n",
    "print('lc_downloaded_LSP_3days:', len(lc_downloaded_LSP_3days))\n",
    "print('lc_downloaded_ISP_3days:', len(lc_downloaded_ISP_3days))\n",
    "print('lc_downloaded_HSP_3days:', len(lc_downloaded_HSP_3days))\n",
    "print('---> total:', len(lc_downloaded_LSP_3days) + len(lc_downloaded_ISP_3days) + len(lc_downloaded_HSP_3days))\n",
    "\n",
    "print('\\nlc_downloaded_3days:', len(lc_downloaded_3days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004e65f-164e-4605-b5ae-7111da02f0a1",
   "metadata": {},
   "source": [
    "To order the numbers correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313201db-5674-4e81-a02c-85b0c0111d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_downloaded_HSP_3days.sort()\n",
    "# lc_downloaded_HSP_3days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2224a01e-450c-443f-be60-2ca3e6c09114",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_downloaded_ISP_3days.sort()\n",
    "# lc_downloaded_ISP_3days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35745bd5-1220-4237-a1d0-85e70de734e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_downloaded_LSP_3days.sort()\n",
    "# lc_downloaded_LSP_3days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a77fe4-db48-47fd-bde6-8457c07d12cc",
   "metadata": {},
   "source": [
    "### Weekly bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0eab184-c2ef-47a9-8491-0847f73a25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select every file that ends with .json in folder\n",
    "lc_weekly = glob.glob(PATH_WEEKLY_LC + '/*.json')\n",
    "\n",
    "## Create empty list to store dataframes\n",
    "lc_downloaded_weekly = []\n",
    "\n",
    "## Loop through list of files and read each one into a dataframe and append to list\n",
    "for file in lc_weekly:\n",
    "    \n",
    "    ## Removing the path from the file name\n",
    "    stripped_file = file.lstrip(f'{PATH_WEEKLY_LC}/')\n",
    "    \n",
    "    ## Append to list of the lightcurves\n",
    "    lc_downloaded_weekly.append(stripped_file)\n",
    "    \n",
    "lc_downloaded_weekly.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa4197f-1a9c-4894-bd5b-e7c0496f357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list for each Synchrotron Type\n",
    "lc_downloaded_LSP_weekly = [] # low\n",
    "lc_downloaded_ISP_weekly = [] # intermediate\n",
    "lc_downloaded_HSP_weekly = [] # high\n",
    "\n",
    "for file in lc_downloaded_weekly:\n",
    "    ## ---------- Low Synchrotron Peak ----------\n",
    "    if (dfLSP['Source_Name'] == file[5:-5]).any():\n",
    "        lc_downloaded_LSP_weekly.append(f'4FGL+{file[5:-5]}.json')\n",
    "        \n",
    "    ## ---------- Intermediate Synchrotron Peak ----------\n",
    "    if (dfISP['Source_Name'] == file[5:-5]).any():\n",
    "        lc_downloaded_ISP_weekly.append(f'4FGL+{file[5:-5]}.json')\n",
    "        \n",
    "    ## ---------- High Synchrotron Peak ----------\n",
    "    if (dfHSP['Source_Name'] == file[5:-5]).any():\n",
    "        lc_downloaded_HSP_weekly.append(f'4FGL+{file[5:-5]}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7ddfb12-4aff-4993-9ecc-2a4ad7fa1543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Weekly bin ---------------\n",
      "\n",
      "# of lightcurves from the folder in the array:\n",
      "lc_downloaded_LSP_weekly: 465\n",
      "lc_downloaded_ISP_weekly: 60\n",
      "lc_downloaded_HSP_weekly: 66\n",
      "---> total: 591\n",
      "\n",
      "lc_downloaded_weekly: 683\n"
     ]
    }
   ],
   "source": [
    "## Making sure we have the right count\n",
    "\n",
    "print('\\n--------------- Weekly bin ---------------')\n",
    "\n",
    "print('\\n# of lightcurves from the folder in the array:')\n",
    "print('lc_downloaded_LSP_weekly:', len(lc_downloaded_LSP_weekly))\n",
    "print('lc_downloaded_ISP_weekly:', len(lc_downloaded_ISP_weekly))\n",
    "print('lc_downloaded_HSP_weekly:', len(lc_downloaded_HSP_weekly))\n",
    "print('---> total:', len(lc_downloaded_LSP_weekly) + len(lc_downloaded_ISP_weekly) + len(lc_downloaded_HSP_weekly))\n",
    "\n",
    "print('\\nlc_downloaded_weekly:', len(lc_downloaded_weekly))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9641d1-ecf5-412e-ac7a-005ab58a9913",
   "metadata": {},
   "source": [
    "To order the numbers correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aac7cd3-58ac-443d-adbb-4cb4ebd4a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_downloaded_HSP_weekly.sort()\n",
    "# lc_downloaded_HSP_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55cb0134-35c9-406e-915e-a6bc433e5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_downloaded_ISP_weekly.sort()\n",
    "# lc_downloaded_ISP_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f892452a-52d4-4e87-b26a-c0e4a554fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_downloaded_LSP_weekly.sort()\n",
    "# lc_downloaded_LSP_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4f092-3b24-449a-9215-4360aa1accb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d5a21-dc5a-456a-9f5a-9e520b8d27b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
